repo,file,code,loops,recursion,function_calls,if_conditions,predicted_complexity
vinta/awesome-python,https://raw.githubusercontent.com/vinta/awesome-python/master/sort.py,"#!/usr/bin/env python
# coding: utf-8

""""""
    The approach taken is explained below. I decided to do it simply.
    Initially I was considering parsing the data into some sort of
    structure and then generating an appropriate README. I am still
    considering doing it - but for now this should work. The only issue
    I see is that it only sorts the entries at the lowest level, and that
    the order of the top-level contents do not match the order of the actual
    entries.

    This could be extended by having nested blocks, sorting them recursively
    and flattening the end structure into a list of lines. Revision 2 maybe ^.^.
""""""

def sort_blocks():
    # First, we load the current README into memory
    with open('README.md', 'r') as read_me_file:
        read_me = read_me_file.read()

    # Separating the 'table of contents' from the contents (blocks)
    table_of_contents = ''.join(read_me.split('- - -')[0])
    blocks = ''.join(read_me.split('- - -')[1]).split('\n# ')
    for i in range(len(blocks)):
        if i == 0:
            blocks[i] = blocks[i] + '\n'
        else:
            blocks[i] = '# ' + blocks[i] + '\n'

    # Sorting the libraries
    inner_blocks = sorted(blocks[0].split('##'))
    for i in range(1, len(inner_blocks)):
        if inner_blocks[i][0] != '#':
            inner_blocks[i] = '##' + inner_blocks[i]
    inner_blocks = ''.join(inner_blocks)

    # Replacing the non-sorted libraries by the sorted ones and gathering all at the final_README file
    blocks[0] = inner_blocks
    final_README = table_of_contents + '- - -' + ''.join(blocks)

    with open('README.md', 'w+') as sorted_file:
        sorted_file.write(final_README)

def main():
    # First, we load the current README into memory as an array of lines
    with open('README.md', 'r') as read_me_file:
        read_me = read_me_file.readlines()

    # Then we cluster the lines together as blocks
    # Each block represents a collection of lines that should be sorted
    # This was done by assuming only links ([...](...)) are meant to be sorted
    # Clustering is done by indentation
    blocks = []
    last_indent = None
    for line in read_me:
        s_line = line.lstrip()
        indent = len(line) - len(s_line)

        if any([s_line.startswith(s) for s in ['* [', '- [']]):
            if indent == last_indent:
                blocks[-1].append(line)
            else:
                blocks.append([line])
            last_indent = indent
        else:
            blocks.append([line])
            last_indent = None

    with open('README.md', 'w+') as sorted_file:
        # Then all of the blocks are sorted individually
        blocks = [
            ''.join(sorted(block, key=str.lower)) for block in blocks
        ]
        # And the result is written back to README.md
        sorted_file.write(''.join(blocks))

    # Then we call the sorting method
    sort_blocks()


if __name__ == ""__main__"":
    main()
",0,0,0,0,4
AUTOMATIC1111/stable-diffusion-webui,https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/launch.py,"from modules import launch_utils

args = launch_utils.args
python = launch_utils.python
git = launch_utils.git
index_url = launch_utils.index_url
dir_repos = launch_utils.dir_repos

commit_hash = launch_utils.commit_hash
git_tag = launch_utils.git_tag

run = launch_utils.run
is_installed = launch_utils.is_installed
repo_dir = launch_utils.repo_dir

run_pip = launch_utils.run_pip
check_run_python = launch_utils.check_run_python
git_clone = launch_utils.git_clone
git_pull_recursive = launch_utils.git_pull_recursive
list_extensions = launch_utils.list_extensions
run_extension_installer = launch_utils.run_extension_installer
prepare_environment = launch_utils.prepare_environment
configure_for_tests = launch_utils.configure_for_tests
start = launch_utils.start


def main():
    if args.dump_sysinfo:
        filename = launch_utils.dump_sysinfo()

        print(f""Sysinfo saved as {filename}. Exiting..."")

        exit(0)

    launch_utils.startup_timer.record(""initial startup"")

    with launch_utils.startup_timer.subcategory(""prepare environment""):
        if not args.skip_prepare_environment:
            prepare_environment()

    if args.test_server:
        configure_for_tests()

    start()


if __name__ == ""__main__"":
    main()
",0,0,0,0,4
AUTOMATIC1111/stable-diffusion-webui,https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/webui.py,"from __future__ import annotations

import os
import time

from modules import timer
from modules import initialize_util
from modules import initialize

startup_timer = timer.startup_timer
startup_timer.record(""launcher"")

initialize.imports()

initialize.check_versions()


def create_api(app):
    from modules.api.api import Api
    from modules.call_queue import queue_lock

    api = Api(app, queue_lock)
    return api


def api_only():
    from fastapi import FastAPI
    from modules.shared_cmd_options import cmd_opts

    initialize.initialize()

    app = FastAPI()
    initialize_util.setup_middleware(app)
    api = create_api(app)

    from modules import script_callbacks
    script_callbacks.before_ui_callback()
    script_callbacks.app_started_callback(None, app)

    print(f""Startup time: {startup_timer.summary()}."")
    api.launch(
        server_name=initialize_util.gradio_server_name(),
        port=cmd_opts.port if cmd_opts.port else 7861,
        root_path=f""/{cmd_opts.subpath}"" if cmd_opts.subpath else """"
    )


def webui():
    from modules.shared_cmd_options import cmd_opts

    launch_api = cmd_opts.api
    initialize.initialize()

    from modules import shared, ui_tempdir, script_callbacks, ui, progress, ui_extra_networks

    while 1:
        if shared.opts.clean_temp_dir_at_start:
            ui_tempdir.cleanup_tmpdr()
            startup_timer.record(""cleanup temp dir"")

        script_callbacks.before_ui_callback()
        startup_timer.record(""scripts before_ui_callback"")

        shared.demo = ui.create_ui()
        startup_timer.record(""create ui"")

        if not cmd_opts.no_gradio_queue:
            shared.demo.queue(64)

        gradio_auth_creds = list(initialize_util.get_gradio_auth_creds()) or None

        auto_launch_browser = False
        if os.getenv('SD_WEBUI_RESTARTING') != '1':
            if shared.opts.auto_launch_browser == ""Remote"" or cmd_opts.autolaunch:
                auto_launch_browser = True
            elif shared.opts.auto_launch_browser == ""Local"":
                auto_launch_browser = not cmd_opts.webui_is_non_local

        app, local_url, share_url = shared.demo.launch(
            share=cmd_opts.share,
            server_name=initialize_util.gradio_server_name(),
            server_port=cmd_opts.port,
            ssl_keyfile=cmd_opts.tls_keyfile,
            ssl_certfile=cmd_opts.tls_certfile,
            ssl_verify=cmd_opts.disable_tls_verify,
            debug=cmd_opts.gradio_debug,
            auth=gradio_auth_creds,
            inbrowser=auto_launch_browser,
            prevent_thread_lock=True,
            allowed_paths=cmd_opts.gradio_allowed_path,
            app_kwargs={
                ""docs_url"": ""/docs"",
                ""redoc_url"": ""/redoc"",
            },
            root_path=f""/{cmd_opts.subpath}"" if cmd_opts.subpath else """",
        )

        startup_timer.record(""gradio launch"")

        # gradio uses a very open CORS policy via app.user_middleware, which makes it possible for
        # an attacker to trick the user into opening a malicious HTML page, which makes a request to the
        # running web ui and do whatever the attacker wants, including installing an extension and
        # running its code. We disable this here. Suggested by RyotaK.
        app.user_middleware = [x for x in app.user_middleware if x.cls.__name__ != 'CORSMiddleware']

        initialize_util.setup_middleware(app)

        progress.setup_progress_api(app)
        ui.setup_ui_api(app)

        if launch_api:
            create_api(app)

        ui_extra_networks.add_pages_to_demo(app)

        startup_timer.record(""add APIs"")

        with startup_timer.subcategory(""app_started_callback""):
            script_callbacks.app_started_callback(shared.demo, app)

        timer.startup_record = startup_timer.dump()
        print(f""Startup time: {startup_timer.summary()}."")

        try:
            while True:
                server_command = shared.state.wait_for_server_command(timeout=5)
                if server_command:
                    if server_command in (""stop"", ""restart""):
                        break
                    else:
                        print(f""Unknown server command: {server_command}"")
        except KeyboardInterrupt:
            print('Caught KeyboardInterrupt, stopping...')
            server_command = ""stop""

        if server_command == ""stop"":
            print(""Stopping server..."")
            # If we catch a keyboard interrupt, we want to stop the server and exit.
            shared.demo.close()
            break

        # disable auto launch webui in browser for subsequent UI Reload
        os.environ.setdefault('SD_WEBUI_RESTARTING', '1')

        print('Restarting UI...')
        shared.demo.close()
        time.sleep(0.5)
        startup_timer.reset()
        script_callbacks.app_reload_callback()
        startup_timer.record(""app reload callback"")
        script_callbacks.script_unloaded_callback()
        startup_timer.record(""scripts unloaded callback"")
        initialize.initialize_rest(reload_script_modules=True)


if __name__ == ""__main__"":
    from modules.shared_cmd_options import cmd_opts

    if cmd_opts.nowebui:
        api_only()
    else:
        webui()
",0,0,0,0,4
huggingface/transformers,https://raw.githubusercontent.com/huggingface/transformers/main/conftest.py,"# Copyright 2020 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# tests directory-specific settings - this file is run automatically
# by pytest before any tests are run

import doctest
import sys
import warnings
from os.path import abspath, dirname, join

import _pytest
import pytest

from transformers.testing_utils import HfDoctestModule, HfDocTestParser


NOT_DEVICE_TESTS = {
    ""test_tokenization"",
    ""test_processor"",
    ""test_processing"",
    ""test_beam_constraints"",
    ""test_configuration_utils"",
    ""test_data_collator"",
    ""test_trainer_callback"",
    ""test_trainer_utils"",
    ""test_feature_extraction"",
    ""test_image_processing"",
    ""test_image_processor"",
    ""test_image_transforms"",
    ""test_optimization"",
    ""test_retrieval"",
    ""test_config"",
    ""test_from_pretrained_no_checkpoint"",
    ""test_keep_in_fp32_modules"",
    ""test_gradient_checkpointing_backward_compatibility"",
    ""test_gradient_checkpointing_enable_disable"",
    ""test_save_load_fast_init_from_base"",
    ""test_fast_init_context_manager"",
    ""test_fast_init_tied_embeddings"",
    ""test_save_load_fast_init_to_base"",
    ""test_torch_save_load"",
    ""test_initialization"",
    ""test_forward_signature"",
    ""test_model_get_set_embeddings"",
    ""test_model_main_input_name"",
    ""test_correct_missing_keys"",
    ""test_tie_model_weights"",
    ""test_can_use_safetensors"",
    ""test_load_save_without_tied_weights"",
    ""test_tied_weights_keys"",
    ""test_model_weights_reload_no_missing_tied_weights"",
    ""test_mismatched_shapes_have_properly_initialized_weights"",
    ""test_matched_shapes_have_loaded_weights_when_some_mismatched_shapes_exist"",
    ""test_model_is_small"",
    ""test_tf_from_pt_safetensors"",
    ""test_flax_from_pt_safetensors"",
    ""ModelTest::test_pipeline_"",  # None of the pipeline tests from PipelineTesterMixin (of which XxxModelTest inherits from) are running on device
    ""ModelTester::test_pipeline_"",
    ""/repo_utils/"",
    ""/utils/"",
    ""/agents/"",
}

# allow having multiple repository checkouts and not needing to remember to rerun
# `pip install -e '.[dev]'` when switching between checkouts and running tests.
git_repo_path = abspath(join(dirname(__file__), ""src""))
sys.path.insert(1, git_repo_path)

# silence FutureWarning warnings in tests since often we can't act on them until
# they become normal warnings - i.e. the tests still need to test the current functionality
warnings.simplefilter(action=""ignore"", category=FutureWarning)


def pytest_configure(config):
    config.addinivalue_line(""markers"", ""is_pipeline_test: mark test to run only when pipelines are tested"")
    config.addinivalue_line(""markers"", ""is_staging_test: mark test to run only in the staging environment"")
    config.addinivalue_line(""markers"", ""accelerate_tests: mark test that require accelerate"")
    config.addinivalue_line(""markers"", ""agent_tests: mark the agent tests that are run on their specific schedule"")
    config.addinivalue_line(""markers"", ""not_device_test: mark the tests always running on cpu"")


def pytest_collection_modifyitems(items):
    for item in items:
        if any(test_name in item.nodeid for test_name in NOT_DEVICE_TESTS):
            item.add_marker(pytest.mark.not_device_test)


def pytest_addoption(parser):
    from transformers.testing_utils import pytest_addoption_shared

    pytest_addoption_shared(parser)


def pytest_terminal_summary(terminalreporter):
    from transformers.testing_utils import pytest_terminal_summary_main

    make_reports = terminalreporter.config.getoption(""--make-reports"")
    if make_reports:
        pytest_terminal_summary_main(terminalreporter, id=make_reports)


def pytest_sessionfinish(session, exitstatus):
    # If no tests are collected, pytest exists with code 5, which makes the CI fail.
    if exitstatus == 5:
        session.exitstatus = 0


# Doctest custom flag to ignore output.
IGNORE_RESULT = doctest.register_optionflag(""IGNORE_RESULT"")

OutputChecker = doctest.OutputChecker


class CustomOutputChecker(OutputChecker):
    def check_output(self, want, got, optionflags):
        if IGNORE_RESULT & optionflags:
            return True
        return OutputChecker.check_output(self, want, got, optionflags)


doctest.OutputChecker = CustomOutputChecker
_pytest.doctest.DoctestModule = HfDoctestModule
doctest.DocTestParser = HfDocTestParser
",0,0,0,0,4
huggingface/transformers,https://raw.githubusercontent.com/huggingface/transformers/main/hubconf.py,"# Copyright 2020 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os
import sys


SRC_DIR = os.path.join(os.path.dirname(__file__), ""src"")
sys.path.append(SRC_DIR)


from transformers import (
    AutoConfig,
    AutoModel,
    AutoModelForCausalLM,
    AutoModelForMaskedLM,
    AutoModelForQuestionAnswering,
    AutoModelForSequenceClassification,
    AutoTokenizer,
    add_start_docstrings,
)


dependencies = [""torch"", ""numpy"", ""tokenizers"", ""filelock"", ""requests"", ""tqdm"", ""regex"", ""sentencepiece"", ""sacremoses"", ""importlib_metadata"", ""huggingface_hub""]


@add_start_docstrings(AutoConfig.__doc__)
def config(*args, **kwargs):
    r""""""
                # Using torch.hub !
                import torch

                config = torch.hub.load('huggingface/transformers', 'config', 'google-bert/bert-base-uncased')  # Download configuration from huggingface.co and cache.
                config = torch.hub.load('huggingface/transformers', 'config', './test/bert_saved_model/')  # E.g. config (or model) was saved using `save_pretrained('./test/saved_model/')`
                config = torch.hub.load('huggingface/transformers', 'config', './test/bert_saved_model/my_configuration.json')
                config = torch.hub.load('huggingface/transformers', 'config', 'google-bert/bert-base-uncased', output_attentions=True, foo=False)
                assert config.output_attentions == True
                config, unused_kwargs = torch.hub.load('huggingface/transformers', 'config', 'google-bert/bert-base-uncased', output_attentions=True, foo=False, return_unused_kwargs=True)
                assert config.output_attentions == True
                assert unused_kwargs == {'foo': False}

            """"""

    return AutoConfig.from_pretrained(*args, **kwargs)


@add_start_docstrings(AutoTokenizer.__doc__)
def tokenizer(*args, **kwargs):
    r""""""
        # Using torch.hub !
        import torch

        tokenizer = torch.hub.load('huggingface/transformers', 'tokenizer', 'google-bert/bert-base-uncased')    # Download vocabulary from huggingface.co and cache.
        tokenizer = torch.hub.load('huggingface/transformers', 'tokenizer', './test/bert_saved_model/')  # E.g. tokenizer was saved using `save_pretrained('./test/saved_model/')`

    """"""

    return AutoTokenizer.from_pretrained(*args, **kwargs)


@add_start_docstrings(AutoModel.__doc__)
def model(*args, **kwargs):
    r""""""
            # Using torch.hub !
            import torch

            model = torch.hub.load('huggingface/transformers', 'model', 'google-bert/bert-base-uncased')    # Download model and configuration from huggingface.co and cache.
            model = torch.hub.load('huggingface/transformers', 'model', './test/bert_model/')  # E.g. model was saved using `save_pretrained('./test/saved_model/')`
            model = torch.hub.load('huggingface/transformers', 'model', 'google-bert/bert-base-uncased', output_attentions=True)  # Update configuration during loading
            assert model.config.output_attentions == True
            # Loading from a TF checkpoint file instead of a PyTorch model (slower)
            config = AutoConfig.from_pretrained('./tf_model/bert_tf_model_config.json')
            model = torch.hub.load('huggingface/transformers', 'model', './tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config)

        """"""

    return AutoModel.from_pretrained(*args, **kwargs)


@add_start_docstrings(AutoModelForCausalLM.__doc__)
def modelForCausalLM(*args, **kwargs):
    r""""""
        # Using torch.hub !
        import torch

        model = torch.hub.load('huggingface/transformers', 'modelForCausalLM', 'openai-community/gpt2')    # Download model and configuration from huggingface.co and cache.
        model = torch.hub.load('huggingface/transformers', 'modelForCausalLM', './test/saved_model/')  # E.g. model was saved using `save_pretrained('./test/saved_model/')`
        model = torch.hub.load('huggingface/transformers', 'modelForCausalLM', 'openai-community/gpt2', output_attentions=True)  # Update configuration during loading
        assert model.config.output_attentions == True
        # Loading from a TF checkpoint file instead of a PyTorch model (slower)
        config = AutoConfig.from_pretrained('./tf_model/gpt_tf_model_config.json')
        model = torch.hub.load('huggingface/transformers', 'modelForCausalLM', './tf_model/gpt_tf_checkpoint.ckpt.index', from_tf=True, config=config)

    """"""
    return AutoModelForCausalLM.from_pretrained(*args, **kwargs)


@add_start_docstrings(AutoModelForMaskedLM.__doc__)
def modelForMaskedLM(*args, **kwargs):
    r""""""
            # Using torch.hub !
            import torch

            model = torch.hub.load('huggingface/transformers', 'modelForMaskedLM', 'google-bert/bert-base-uncased')    # Download model and configuration from huggingface.co and cache.
            model = torch.hub.load('huggingface/transformers', 'modelForMaskedLM', './test/bert_model/')  # E.g. model was saved using `save_pretrained('./test/saved_model/')`
            model = torch.hub.load('huggingface/transformers', 'modelForMaskedLM', 'google-bert/bert-base-uncased', output_attentions=True)  # Update configuration during loading
            assert model.config.output_attentions == True
            # Loading from a TF checkpoint file instead of a PyTorch model (slower)
            config = AutoConfig.from_pretrained('./tf_model/bert_tf_model_config.json')
            model = torch.hub.load('huggingface/transformers', 'modelForMaskedLM', './tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config)

        """"""

    return AutoModelForMaskedLM.from_pretrained(*args, **kwargs)


@add_start_docstrings(AutoModelForSequenceClassification.__doc__)
def modelForSequenceClassification(*args, **kwargs):
    r""""""
            # Using torch.hub !
            import torch

            model = torch.hub.load('huggingface/transformers', 'modelForSequenceClassification', 'google-bert/bert-base-uncased')    # Download model and configuration from huggingface.co and cache.
            model = torch.hub.load('huggingface/transformers', 'modelForSequenceClassification', './test/bert_model/')  # E.g. model was saved using `save_pretrained('./test/saved_model/')`
            model = torch.hub.load('huggingface/transformers', 'modelForSequenceClassification', 'google-bert/bert-base-uncased', output_attentions=True)  # Update configuration during loading
            assert model.config.output_attentions == True
            # Loading from a TF checkpoint file instead of a PyTorch model (slower)
            config = AutoConfig.from_pretrained('./tf_model/bert_tf_model_config.json')
            model = torch.hub.load('huggingface/transformers', 'modelForSequenceClassification', './tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config)

        """"""

    return AutoModelForSequenceClassification.from_pretrained(*args, **kwargs)


@add_start_docstrings(AutoModelForQuestionAnswering.__doc__)
def modelForQuestionAnswering(*args, **kwargs):
    r""""""
        # Using torch.hub !
        import torch

        model = torch.hub.load('huggingface/transformers', 'modelForQuestionAnswering', 'google-bert/bert-base-uncased')    # Download model and configuration from huggingface.co and cache.
        model = torch.hub.load('huggingface/transformers', 'modelForQuestionAnswering', './test/bert_model/')  # E.g. model was saved using `save_pretrained('./test/saved_model/')`
        model = torch.hub.load('huggingface/transformers', 'modelForQuestionAnswering', 'google-bert/bert-base-uncased', output_attentions=True)  # Update configuration during loading
        assert model.config.output_attentions == True
        # Loading from a TF checkpoint file instead of a PyTorch model (slower)
        config = AutoConfig.from_pretrained('./tf_model/bert_tf_model_config.json')
        model = torch.hub.load('huggingface/transformers', 'modelForQuestionAnswering', './tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config)

    """"""
    return AutoModelForQuestionAnswering.from_pretrained(*args, **kwargs)
",0,0,0,0,4
huggingface/transformers,https://raw.githubusercontent.com/huggingface/transformers/main/setup.py,"# Copyright 2021 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

""""""
Simple check list from AllenNLP repo: https://github.com/allenai/allennlp/blob/main/setup.py

To create the package for pypi.

1. Create the release branch named: v<RELEASE>-release, for example v4.19-release. For a patch release checkout the
   current release branch.

   If releasing on a special branch, copy the updated README.md on the main branch for the commit you will make
   for the post-release and run `make fix-copies` on the main branch as well.

2. Run `make pre-release` (or `make pre-patch` for a patch release) and commit these changes with the message:
   ""Release: <VERSION>"" and push.

3. Go back to the main branch and run `make post-release` then `make fix-copies`. Commit these changes with the
   message ""v<NEXT_VERSION>.dev.0"" and push to main.

# If you were just cutting the branch in preparation for a release, you can stop here for now.

4. Wait for the tests on the release branch to be completed and be green (otherwise revert and fix bugs)

5. On the release branch, add a tag in git to mark the release: ""git tag v<VERSION> -m 'Adds tag v<VERSION> for pypi' ""
   Push the tag to git: git push --tags origin v<RELEASE>-release

6. Build both the sources and the wheel. Do not change anything in setup.py between
   creating the wheel and the source distribution (obviously).

   Run `make build-release`. This will build the release and do some sanity checks for you. If this ends with an error
   message, you need to fix things before going further.

   You should now have a /dist directory with both .whl and .tar.gz source versions.

7. Check that everything looks correct by uploading the package to the pypi test server:

   twine upload dist/* -r testpypi
   (pypi suggest using twine as other methods upload files via plaintext.)
   You may have to specify the repository url, use the following command then:
   twine upload dist/* -r testpypi --repository-url=https://test.pypi.org/legacy/

   Check that you can install it in a virtualenv by running:
   pip install -i https://testpypi.python.org/pypi transformers

   Check you can run the following commands:
   python -c ""from transformers import pipeline; classifier = pipeline('text-classification'); print(classifier('What a nice release'))""
   python -c ""from transformers import *""
   python utils/check_build.py --check_lib

   If making a patch release, double check the bug you are patching is indeed resolved.

8. Upload the final version to actual pypi:
   twine upload dist/* -r pypi

9. Copy the release notes from RELEASE.md to the tag in github once everything is looking hunky-dory.
""""""

import os
import re
import shutil
from pathlib import Path

from setuptools import Command, find_packages, setup


# Remove stale transformers.egg-info directory to avoid https://github.com/pypa/pip/issues/5466
stale_egg_info = Path(__file__).parent / ""transformers.egg-info""
if stale_egg_info.exists():
    print(
        (
            ""Warning: {} exists.\n\n""
            ""If you recently updated transformers to 3.0 or later, this is expected,\n""
            ""but it may prevent transformers from installing in editable mode.\n\n""
            ""This directory is automatically generated by Python's packaging tools.\n""
            ""I will remove it now.\n\n""
            ""See https://github.com/pypa/pip/issues/5466 for details.\n""
        ).format(stale_egg_info)
    )
    shutil.rmtree(stale_egg_info)


# IMPORTANT:
# 1. all dependencies should be listed here with their version requirements if any
# 2. once modified, run: `make deps_table_update` to update src/transformers/dependency_versions_table.py
_deps = [
    ""Pillow>=10.0.1,<=15.0"",
    ""accelerate>=0.26.0"",
    ""av"",
    ""beautifulsoup4"",
    ""blobfile"",
    ""codecarbon>=2.8.1"",
    ""cookiecutter==1.7.3"",
    ""dataclasses"",
    ""datasets!=2.5.0"",
    ""deepspeed>=0.9.3"",
    ""diffusers"",
    ""dill<0.3.5"",
    ""evaluate>=0.2.0"",
    ""faiss-cpu"",
    ""fastapi"",
    ""filelock"",
    ""flax>=0.4.1,<=0.7.0"",
    ""fsspec<2023.10.0"",
    ""ftfy"",
    ""fugashi>=1.0"",
    ""GitPython<3.1.19"",
    ""hf-doc-builder>=0.3.0"",
    ""huggingface-hub>=0.26.0,<1.0"",
    ""importlib_metadata"",
    ""ipadic>=1.0.0,<2.0"",
    ""isort>=5.5.4"",
    ""jax>=0.4.1,<=0.4.13"",
    ""jaxlib>=0.4.1,<=0.4.13"",
    ""jieba"",
    ""jinja2>=3.1.0"",
    ""kenlm"",
    # Keras pin - this is to make sure Keras 3 doesn't destroy us. Remove or change when we have proper support.
    ""keras>2.9,<2.16"",
    ""keras-nlp>=0.3.1,<0.14.0"",  # keras-nlp 0.14 doesn't support keras 2, see pin on keras.
    ""librosa"",
    ""natten>=0.14.6,<0.15.0"",
    ""nltk<=3.8.1"",
    ""num2words"",
    ""numpy>=1.17"",
    ""onnxconverter-common"",
    ""onnxruntime-tools>=1.4.2"",
    ""onnxruntime>=1.4.0"",
    ""opencv-python"",
    ""optimum-benchmark>=0.3.0"",
    ""optuna"",
    ""optax>=0.0.8,<=0.1.4"",
    ""packaging>=20.0"",
    ""parameterized"",
    ""phonemizer"",
    ""protobuf"",
    ""psutil"",
    ""pyyaml>=5.1"",
    ""pydantic"",
    ""pytest>=7.2.0,<8.0.0"",
    ""pytest-asyncio"",
    ""pytest-timeout"",
    ""pytest-xdist"",
    ""python>=3.9.0"",
    ""ray[tune]>=2.7.0"",
    ""regex!=2019.12.17"",
    ""requests"",
    ""rhoknp>=1.1.0,<1.3.1"",
    ""rjieba"",
    ""rouge-score!=0.0.7,!=0.0.8,!=0.1,!=0.1.1"",
    ""ruff==0.5.1"",
    ""sacrebleu>=1.4.12,<2.0.0"",
    ""sacremoses"",
    ""safetensors>=0.4.1"",
    ""sagemaker>=2.31.0"",
    ""schedulefree>=1.2.6"",
    ""scikit-learn"",
    ""scipy<1.13.0"",  # SciPy >= 1.13.0 is not supported with the current jax pin (`jax>=0.4.1,<=0.4.13`)
    ""sentencepiece>=0.1.91,!=0.1.92"",
    ""sigopt"",
    ""starlette"",
    ""sudachipy>=0.6.6"",
    ""sudachidict_core>=20220729"",
    ""tensorboard"",
    # TensorFlow pin. When changing this value, update examples/tensorflow/_tests_requirements.txt accordingly
    ""tensorflow-cpu>2.9,<2.16"",
    ""tensorflow>2.9,<2.16"",
    ""tensorflow-text<2.16"",
    ""tensorflow-probability<0.24"",
    ""tf2onnx"",
    ""timeout-decorator"",
    ""tiktoken"",
    ""timm<=1.0.11"",
    ""tokenizers>=0.21,<0.22"",
    ""torch>=2.0"",
    ""torchaudio"",
    ""torchvision"",
    ""pyctcdecode>=0.4.0"",
    ""tqdm>=4.27"",
    ""unidic>=1.0.2"",
    ""unidic_lite>=1.0.7"",
    ""urllib3<2.0.0"",
    ""uvicorn"",
    ""pytest-rich"",
    ""libcst"",
    ""rich"",
]


# this is a lookup table with items like:
#
# tokenizers: ""tokenizers==0.9.4""
# packaging: ""packaging""
#
# some of the values are versioned whereas others aren't.
deps = {b: a for a, b in (re.findall(r""^(([^!=<>~ ]+)(?:[!=<>~ ].*)?$)"", x)[0] for x in _deps)}

# since we save this data in src/transformers/dependency_versions_table.py it can be easily accessed from
# anywhere. If you need to quickly access the data from this table in a shell, you can do so easily with:
#
# python -c 'import sys; from transformers.dependency_versions_table import deps; \
# print("" "".join([ deps[x] for x in sys.argv[1:]]))' tokenizers datasets
#
# Just pass the desired package names to that script as it's shown with 2 packages above.
#
# If transformers is not yet installed and the work is done from the cloned repo remember to add `PYTHONPATH=src` to the script above
#
# You can then feed this for example to `pip`:
#
# pip install -U $(python -c 'import sys; from transformers.dependency_versions_table import deps; \
# print("" "".join([deps[x] for x in sys.argv[1:]]))' tokenizers datasets)
#


def deps_list(*pkgs):
    return [deps[pkg] for pkg in pkgs]


class DepsTableUpdateCommand(Command):
    """"""
    A custom distutils command that updates the dependency table.
    usage: python setup.py deps_table_update
    """"""

    description = ""build runtime dependency table""
    user_options = [
        # format: (long option, short option, description).
        (""dep-table-update"", None, ""updates src/transformers/dependency_versions_table.py""),
    ]

    def initialize_options(self):
        pass

    def finalize_options(self):
        pass

    def run(self):
        entries = ""\n"".join([f'    ""{k}"": ""{v}"",' for k, v in deps.items()])
        content = [
            ""# THIS FILE HAS BEEN AUTOGENERATED. To update:"",
            ""# 1. modify the `_deps` dict in setup.py"",
            ""# 2. run `make deps_table_update``"",
            ""deps = {"",
            entries,
            ""}"",
            """",
        ]
        target = ""src/transformers/dependency_versions_table.py""
        print(f""updating {target}"")
        with open(target, ""w"", encoding=""utf-8"", newline=""\n"") as f:
            f.write(""\n"".join(content))


extras = {}

extras[""ja""] = deps_list(""fugashi"", ""ipadic"", ""unidic_lite"", ""unidic"", ""sudachipy"", ""sudachidict_core"", ""rhoknp"")
extras[""sklearn""] = deps_list(""scikit-learn"")

extras[""tf""] = deps_list(""tensorflow"", ""onnxconverter-common"", ""tf2onnx"", ""tensorflow-text"", ""keras-nlp"")
extras[""tf-cpu""] = deps_list(
    ""keras"",
    ""tensorflow-cpu"",
    ""onnxconverter-common"",
    ""tf2onnx"",
    ""tensorflow-text"",
    ""keras-nlp"",
    ""tensorflow-probability"",
)

extras[""torch""] = deps_list(""torch"", ""accelerate"")
extras[""accelerate""] = deps_list(""accelerate"")

if os.name == ""nt"":  # windows
    extras[""retrieval""] = deps_list(""datasets"")  # faiss is not supported on windows
    extras[""flax""] = []  # jax is not supported on windows
else:
    extras[""retrieval""] = deps_list(""faiss-cpu"", ""datasets"")
    extras[""flax""] = deps_list(""jax"", ""jaxlib"", ""flax"", ""optax"", ""scipy"")

extras[""tokenizers""] = deps_list(""tokenizers"")
extras[""ftfy""] = deps_list(""ftfy"")
extras[""onnxruntime""] = deps_list(""onnxruntime"", ""onnxruntime-tools"")
extras[""onnx""] = deps_list(""onnxconverter-common"", ""tf2onnx"") + extras[""onnxruntime""]
extras[""modelcreation""] = deps_list(""cookiecutter"")

extras[""sagemaker""] = deps_list(""sagemaker"")
extras[""deepspeed""] = deps_list(""deepspeed"") + extras[""accelerate""]
extras[""optuna""] = deps_list(""optuna"")
extras[""ray""] = deps_list(""ray[tune]"")
extras[""sigopt""] = deps_list(""sigopt"")

extras[""integrations""] = extras[""optuna""] + extras[""ray""] + extras[""sigopt""]

extras[""serving""] = deps_list(""pydantic"", ""uvicorn"", ""fastapi"", ""starlette"")
extras[""audio""] = deps_list(""librosa"", ""pyctcdecode"", ""phonemizer"", ""kenlm"")
# `pip install "".[speech]""` is deprecated and `pip install "".[torch-speech]""` should be used instead
extras[""speech""] = deps_list(""torchaudio"") + extras[""audio""]
extras[""torch-speech""] = deps_list(""torchaudio"") + extras[""audio""]
extras[""tf-speech""] = extras[""audio""]
extras[""flax-speech""] = extras[""audio""]
extras[""vision""] = deps_list(""Pillow"")
extras[""timm""] = deps_list(""timm"")
extras[""torch-vision""] = deps_list(""torchvision"") + extras[""vision""]
extras[""natten""] = deps_list(""natten"")
extras[""codecarbon""] = deps_list(""codecarbon"")
extras[""video""] = deps_list(""av"")
extras[""num2words""] = deps_list(""num2words"")
extras[""sentencepiece""] = deps_list(""sentencepiece"", ""protobuf"")
extras[""tiktoken""] = deps_list(""tiktoken"", ""blobfile"")
extras[""testing""] = (
    deps_list(
        ""pytest"",
        ""pytest-asyncio"",
        ""pytest-rich"",
        ""pytest-xdist"",
        ""timeout-decorator"",
        ""parameterized"",
        ""psutil"",
        ""datasets"",
        ""dill"",
        ""evaluate"",
        ""pytest-timeout"",
        ""ruff"",
        ""sacrebleu"",
        ""rouge-score"",
        ""nltk"",
        ""GitPython"",
        ""sacremoses"",
        ""rjieba"",
        ""beautifulsoup4"",
        ""tensorboard"",
        ""pydantic"",
        ""sentencepiece"",
    )
    + extras[""retrieval""]
    + extras[""modelcreation""]
)

extras[""deepspeed-testing""] = extras[""deepspeed""] + extras[""testing""] + extras[""optuna""] + extras[""sentencepiece""]
extras[""ruff""] = deps_list(""ruff"")
extras[""quality""] = deps_list(""datasets"", ""isort"", ""ruff"", ""GitPython"", ""urllib3"", ""libcst"", ""rich"")

extras[""all""] = (
    extras[""tf""]
    + extras[""torch""]
    + extras[""flax""]
    + extras[""sentencepiece""]
    + extras[""tokenizers""]
    + extras[""torch-speech""]
    + extras[""vision""]
    + extras[""integrations""]
    + extras[""timm""]
    + extras[""torch-vision""]
    + extras[""codecarbon""]
    + extras[""accelerate""]
    + extras[""video""]
    + extras[""num2words""]
)


extras[""dev-torch""] = (
    extras[""testing""]
    + extras[""torch""]
    + extras[""sentencepiece""]
    + extras[""tokenizers""]
    + extras[""torch-speech""]
    + extras[""vision""]
    + extras[""integrations""]
    + extras[""timm""]
    + extras[""torch-vision""]
    + extras[""codecarbon""]
    + extras[""quality""]
    + extras[""ja""]
    + extras[""sklearn""]
    + extras[""modelcreation""]
    + extras[""onnxruntime""]
    + extras[""num2words""]
)
extras[""dev-tensorflow""] = (
    extras[""testing""]
    + extras[""tf""]
    + extras[""sentencepiece""]
    + extras[""tokenizers""]
    + extras[""vision""]
    + extras[""quality""]
    + extras[""sklearn""]
    + extras[""modelcreation""]
    + extras[""onnx""]
    + extras[""tf-speech""]
)
extras[""dev""] = (
    extras[""all""] + extras[""testing""] + extras[""quality""] + extras[""ja""] + extras[""sklearn""] + extras[""modelcreation""]
)

extras[""torchhub""] = deps_list(
    ""filelock"",
    ""huggingface-hub"",
    ""importlib_metadata"",
    ""numpy"",
    ""packaging"",
    ""protobuf"",
    ""regex"",
    ""requests"",
    ""sentencepiece"",
    ""torch"",
    ""tokenizers"",
    ""tqdm"",
)

extras[""agents""] = deps_list(
    ""diffusers"", ""accelerate"", ""datasets"", ""torch"", ""sentencepiece"", ""opencv-python"", ""Pillow""
)

extras[""benchmark""] = deps_list(""optimum-benchmark"")

# when modifying the following list, make sure to update src/transformers/dependency_versions_check.py
install_requires = [
    deps[""filelock""],  # filesystem locks, e.g., to prevent parallel downloads
    deps[""huggingface-hub""],
    deps[""numpy""],
    deps[""packaging""],  # utilities from PyPA to e.g., compare versions
    deps[""pyyaml""],  # used for the model cards metadata
    deps[""regex""],  # for OpenAI GPT
    deps[""requests""],  # for downloading models over HTTPS
    deps[""tokenizers""],
    deps[""safetensors""],
    deps[""tqdm""],  # progress bars in model download and training scripts
]

setup(
    name=""transformers"",
    version=""4.50.0.dev0"",  # expected format is one of x.y.z.dev0, or x.y.z.rc1 or x.y.z (no to dashes, yes to dots)
    author=""The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)"",
    author_email=""transformers@huggingface.co"",
    description=""State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow"",
    long_description=open(""README.md"", ""r"", encoding=""utf-8"").read(),
    long_description_content_type=""text/markdown"",
    keywords=""NLP vision speech deep learning transformer pytorch tensorflow jax BERT GPT-2 Wav2Vec2 ViT"",
    license=""Apache 2.0 License"",
    url=""https://github.com/huggingface/transformers"",
    package_dir={"""": ""src""},
    packages=find_packages(""src""),
    include_package_data=True,
    package_data={"""": [""**/*.cu"", ""**/*.cpp"", ""**/*.cuh"", ""**/*.h"", ""**/*.pyx""]},
    zip_safe=False,
    extras_require=extras,
    entry_points={""console_scripts"": [""transformers-cli=transformers.commands.transformers_cli:main""]},
    python_requires="">=3.9.0"",
    install_requires=list(install_requires),
    classifiers=[
        ""Development Status :: 5 - Production/Stable"",
        ""Intended Audience :: Developers"",
        ""Intended Audience :: Education"",
        ""Intended Audience :: Science/Research"",
        ""License :: OSI Approved :: Apache Software License"",
        ""Operating System :: OS Independent"",
        ""Programming Language :: Python :: 3"",
        ""Programming Language :: Python :: 3.9"",
        ""Programming Language :: Python :: 3.10"",
        ""Topic :: Scientific/Engineering :: Artificial Intelligence"",
    ],
    cmdclass={""deps_table_update"": DepsTableUpdateCommand},
)

extras[""tests_torch""] = deps_list()
extras[""tests_tf""] = deps_list()
extras[""tests_flax""] = deps_list()
extras[""tests_hub""] = deps_list()
extras[""tests_pipelines_torch""] = deps_list()
extras[""tests_pipelines_tf""] = deps_list()
extras[""tests_onnx""] = deps_list()
extras[""tests_examples_torch""] = deps_list()
extras[""tests_examples_tf""] = deps_list()
extras[""tests_custom_tokenizers""] = deps_list()
extras[""tests_exotic_models""] = deps_list()
extras[""consistency""] = deps_list()
",0,0,0,0,4
ytdl-org/youtube-dl,https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/setup.py,"#!/usr/bin/env python
# coding: utf-8

from __future__ import print_function

import os.path
import warnings
import sys

try:
    from setuptools import setup, Command
    setuptools_available = True
except ImportError:
    from distutils.core import setup, Command
    setuptools_available = False
from distutils.spawn import spawn

try:
    # This will create an exe that needs Microsoft Visual C++ 2008
    # Redistributable Package
    import py2exe
except ImportError:
    if len(sys.argv) >= 2 and sys.argv[1] == 'py2exe':
        print('Cannot import py2exe', file=sys.stderr)
        exit(1)

py2exe_options = {
    'bundle_files': 1,
    'compressed': 1,
    'optimize': 2,
    'dist_dir': '.',
    'dll_excludes': ['w9xpopen.exe', 'crypt32.dll'],
}

# Get the version from youtube_dl/version.py without importing the package
exec(compile(open('youtube_dl/version.py').read(),
             'youtube_dl/version.py', 'exec'))

DESCRIPTION = 'YouTube video downloader'
LONG_DESCRIPTION = 'Command-line program to download videos from YouTube.com and other video sites'

py2exe_console = [{
    'script': './youtube_dl/__main__.py',
    'dest_base': 'youtube-dl',
    'version': __version__,
    'description': DESCRIPTION,
    'comments': LONG_DESCRIPTION,
    'product_name': 'youtube-dl',
    'product_version': __version__,
}]

py2exe_params = {
    'console': py2exe_console,
    'options': {'py2exe': py2exe_options},
    'zipfile': None
}

if len(sys.argv) >= 2 and sys.argv[1] == 'py2exe':
    params = py2exe_params
else:
    files_spec = [
        ('etc/bash_completion.d', ['youtube-dl.bash-completion']),
        ('etc/fish/completions', ['youtube-dl.fish']),
        ('share/doc/youtube_dl', ['README.txt']),
        ('share/man/man1', ['youtube-dl.1'])
    ]
    root = os.path.dirname(os.path.abspath(__file__))
    data_files = []
    for dirname, files in files_spec:
        resfiles = []
        for fn in files:
            if not os.path.exists(fn):
                warnings.warn('Skipping file %s since it is not present. Type  make  to build all automatically generated files.' % fn)
            else:
                resfiles.append(fn)
        data_files.append((dirname, resfiles))

    params = {
        'data_files': data_files,
    }
    if setuptools_available:
        params['entry_points'] = {'console_scripts': ['youtube-dl = youtube_dl:main']}
    else:
        params['scripts'] = ['bin/youtube-dl']

class build_lazy_extractors(Command):
    description = 'Build the extractor lazy loading module'
    user_options = []

    def initialize_options(self):
        pass

    def finalize_options(self):
        pass

    def run(self):
        spawn(
            [sys.executable, 'devscripts/make_lazy_extractors.py', 'youtube_dl/extractor/lazy_extractors.py'],
            dry_run=self.dry_run,
        )

setup(
    name='youtube_dl',
    version=__version__,
    description=DESCRIPTION,
    long_description=LONG_DESCRIPTION,
    url='https://github.com/ytdl-org/youtube-dl',
    author='Ricardo Garcia',
    author_email='ytdl@yt-dl.org',
    maintainer='Sergey M.',
    maintainer_email='dstftw@gmail.com',
    license='Unlicense',
    packages=[
        'youtube_dl',
        'youtube_dl.extractor', 'youtube_dl.downloader',
        'youtube_dl.postprocessor'],

    # Provokes warning on most systems (why?!)
    # test_suite = 'nose.collector',
    # test_requires = ['nosetest'],

    classifiers=[
        'Topic :: Multimedia :: Video',
        'Development Status :: 5 - Production/Stable',
        'Environment :: Console',
        'License :: Public Domain',
        'Programming Language :: Python',
        'Programming Language :: Python :: 2',
        'Programming Language :: Python :: 2.6',
        'Programming Language :: Python :: 2.7',
        'Programming Language :: Python :: 3',
        'Programming Language :: Python :: 3.2',
        'Programming Language :: Python :: 3.3',
        'Programming Language :: Python :: 3.4',
        'Programming Language :: Python :: 3.5',
        'Programming Language :: Python :: 3.6',
        'Programming Language :: Python :: 3.7',
        'Programming Language :: Python :: 3.8',
        'Programming Language :: Python :: Implementation',
        'Programming Language :: Python :: Implementation :: CPython',
        'Programming Language :: Python :: Implementation :: IronPython',
        'Programming Language :: Python :: Implementation :: Jython',
        'Programming Language :: Python :: Implementation :: PyPy',
    ],

    cmdclass={'build_lazy_extractors': build_lazy_extractors},
    **params
)
",0,0,0,0,4
